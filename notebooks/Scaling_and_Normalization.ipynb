{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/logo.png'>\n",
    "<img src='img/title.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [Preprocessing](#Preprocessing)\n",
    "\t* [Setup](#Setup)\n",
    "\t* [Example workflow with `MinMaxScaler`](#Example-workflow-with-MinMaxScaler)\n",
    "\t* [Scaling training and test data the same way](#Scaling-training-and-test-data-the-same-way)\n",
    "\t* [The effect of preprocessing on supervised learning](#The-effect-of-preprocessing-on-supervised-learning)\n",
    "\t\t* [`preprocessing.scale`](#preprocessing.scale)\n",
    "* [Summary](#Summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fix working directory\n",
    "%cd notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import holoviews as hv\n",
    "import panel as pn\n",
    "hv.extension('bokeh')\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['image.interpolation'] = \"none\"\n",
    "np.set_printoptions(precision=3)\n",
    "plt.rcParams['image.cmap'] = \"gray\"\n",
    "\n",
    "import src.mglearn as mglearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing\n",
    "\n",
    "Preprocessing is often desirable for the following reasons:\n",
    " * Input data may have many dimensions and/or colinear dimensions, making it desirable to simplify the fitting problem by reducing the number of columns (features)\n",
    " * Input data may not have the statistical properties that are ideal for fitting\n",
    "\n",
    "The following cell shows how several different preprocessing scalers affect input feature statstics.  An overview of the [sklearn.preprocessing api can be found here](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing).\n",
    "\n",
    "The block beginning with:\n",
    "```\n",
    "\n",
    "for scaler in [StandardScaler(), RobustScaler(),\n",
    "               MinMaxScaler(), Normalizer(norm='l2')]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is looping over 4 different scalers:\n",
    " * `StandardScaler`: [Remove mean and create unit variance](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
    " * `RobustScaler`: [Scaling robust to outliers in input data](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler)\n",
    " * `MinMaxScaler`: [Scale to the min / max range of each feature](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler)\n",
    " * `Normalizer(norm='l2')`: [Normalize to a unit norm](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html#sklearn.preprocessing.Normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer, RobustScaler\n",
    "\n",
    "\n",
    "X, y = make_blobs(n_samples=50, centers=2, random_state=4, cluster_std=1)\n",
    "\n",
    "# move away from origin\n",
    "X += 3\n",
    "\n",
    "# add an outlier\n",
    "X = np.append(X, [[17,11.5]], axis=0)\n",
    "y = np.append(y, 1)\n",
    "\n",
    "scaled = []\n",
    "for scaler in [StandardScaler(), RobustScaler(),\n",
    "               MinMaxScaler(), Normalizer(norm='l2')]:\n",
    "    scaled_X = scaler.fit_transform(X)\n",
    "    scaled.append((scaler.__class__.__name__, scaled_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_options=dict(color='Category', cmap='Category10', size=5)\n",
    "\n",
    "original = (hv.Points((X[:,0],X[:,1],y), vdims=['Category'])\n",
    "            .options(**plot_options, xlim=(-18,18), ylim=(-12,12), title='Original Data', width=450, height=450)\n",
    "           )\n",
    "\n",
    "scaled_plots = []\n",
    "for name, scaled_X in scaled:\n",
    "    p = (hv.Points((scaled_X[:,0],scaled_X[:,1],y), vdims=['Category'])\n",
    "         .options(**plot_options, xlim=(-3,3), ylim=(-3,3), title=name, width=250, height=250)\n",
    "        )\n",
    "    \n",
    "    scaled_plots.append(p)\n",
    "\n",
    "scaled_plots = pn.Column(pn.Row(*scaled_plots[:2]), pn.Row(*scaled_plots[2:]))\n",
    "pn.Row(original, scaled_plots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example workflow with `MinMaxScaler`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example uses a single training data / test data split, with a `MinMaxScaler` putting features in the `(0, 1)` range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target,\n",
    "                                                    random_state=1)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(X_train)    # first .fit finds stats needed for scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stats of the data before / after MinMaxScaler\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "print(\"transformed shape: %s\" % (X_train_scaled.shape,))\n",
    "print(\"per-feature minimum before scaling:\\n %s\" % X_train.min(axis=0))\n",
    "print(\"per-feature maximum before scaling:\\n %s\" % X_train.max(axis=0))\n",
    "print(\"per-feature minimum after scaling:\\n %s\" % X_train_scaled.min(axis=0))\n",
    "print(\"per-feature maximum after scaling:\\n %s\" % X_train_scaled.max(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform test data\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "# print test data properties after scaling\n",
    "print(\"per-feature minimum after scaling: %s\" % X_test_scaled.min(axis=0))\n",
    "print(\"per-feature maximum after scaling: %s\" % X_test_scaled.max(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling training and test data the same way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to call the scaler's `.fit` method separately for training and test data, so the data are not scaled by a different data set's statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "# make synthetic data\n",
    "X, _ = make_blobs(n_samples=25, centers=5, random_state=5, cluster_std=2)\n",
    "# split it into training and test set\n",
    "X_train, X_test = train_test_split(X, random_state=5, test_size=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescale the test set separately, so that test set min is 0 and test set max is 1\n",
    "# DO NOT DO THIS! For illustration purposes only\n",
    "test_scaler = MinMaxScaler()\n",
    "\n",
    "test_scaler.fit(X_test)\n",
    "X_test_scaled_badly = test_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_options = dict(size=6, padding=0.5, legend_position='top_right', height=330, width=330)\n",
    "\n",
    "unscaled = hv.Points(X_train, label='Training Set').options(**plot_options) * \\\n",
    "           hv.Points(X_test, label='Test Set').options(**plot_options, title='Unscaled')\n",
    "\n",
    "correct_scaling = hv.Points(X_train_scaled, label='Training Set').options(**plot_options) * \\\n",
    "                  hv.Points(X_test_scaled, label='Test Set').options(**plot_options, title='Correct')\n",
    "\n",
    "bad_scaling = hv.Points(X_train_scaled, label='Training Set').options(**plot_options) * \\\n",
    "              hv.Points(X_test_scaled_badly, label='Test Set').options(**plot_options, title='Incorrect')\n",
    "\n",
    "pn.Row(unscaled, correct_scaling, bad_scaling)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The effect of preprocessing on supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells below show that min / max scaling or standardization can improve the predictive value of a support vector classifier for the cancer data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target,\n",
    "                                                    random_state=0)\n",
    "\n",
    "svm = SVC(C=100)\n",
    "svm.fit(X_train, y_train)\n",
    "print(svm.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing using 0-1 scaling\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# learning an SVM on the scaled training data\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "# scoring on the scaled test set\n",
    "\n",
    "svm.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing using zero mean and unit variance scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# learning an SVM on the scaled training data\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "# scoring on the scaled test set\n",
    "svm.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `preprocessing.scale`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling with function instead of a class.  The default behavior removes the mean and scales standard deviation to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "train = np.c_[np.random.normal(10, 2, 100),\n",
    "              np.random.normal(size=100),\n",
    "              np.random.normal(0, 5, 100)]\n",
    "\n",
    "# with_mean/with_std True/False (default True)\n",
    "scaled = preprocessing.scale(train)\n",
    "print(np.allclose(0.0, scaled.mean(axis=0)), np.allclose(1.0, scaled.std(axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we reviewed the following topics in preparation for more advanced topics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Preprocessing](#Preprocessing)\n",
    "* [Example workflow with `MinMaxScaler`](#Example-workflow-with-MinMaxScaler)\n",
    "* [Scaling training and test data the same way](#Scaling-training-and-test-data-the-same-way)\n",
    "* [The effect of preprocessing on supervised learning](#The-effect-of-preprocessing-on-supervised-learning)\n",
    "* [`preprocessing.scale`](#preprocessing.scale)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='Scaling_and_Normalization_Exercises.ipynb' class='btn btn-primary btn-lg'>Exercises</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/copyright.png'>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 [module-machine-learning]",
   "language": "python",
   "name": "anaconda-project-module-machine-learning-python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
