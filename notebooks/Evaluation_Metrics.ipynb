{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/logo.png'>\n",
    "<img src='img/title.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook describes how to quantitatively assess models' information value by calculating classification metrics like receiver operating characteristic (ROC) curves and confusion matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [Classification](#Classification)\n",
    "* [Evaluation metrics and scoring](#Evaluation-metrics-and-scoring)\n",
    "\t* [Metrics for binary classification](#Metrics-for-binary-classification)\n",
    "\t\t* [Confusion matrices](#Confusion-matrices)\n",
    "\t\t\t* [Relation to accuracy](#Relation-to-accuracy)\n",
    "\t\t* [Precision, recall and f-score](#Precision,-recall-and-f-score)\n",
    "* [Taking uncertainty into account](#Taking-uncertainty-into-account)\n",
    "\t* [Precision-Recall curves and ROC curves](#Precision-Recall-curves-and-ROC-curves)\n",
    "\t\t* [Exercise: compare cancer models](#Exercise:-compare-cancer-models)\n",
    "\t\t* [ROC and AUC](#ROC-and-AUC)\n",
    "\t* [Multi-class classification](#Multi-class-classification)\n",
    "* [Summary](#Summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fix working directory\n",
    "%cd notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import holoviews as hv\n",
    "import hvplot.pandas\n",
    "import panel as pn\n",
    "hv.extension('bokeh')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['image.interpolation'] = \"none\"\n",
    "np.set_printoptions(precision=3)\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['legend.numpoints'] = 1\n",
    "\n",
    "import src.mglearn as mglearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification is the set of techniques to predict a label, either binary or from a set, based on the features.\n",
    "\n",
    "Simple classifiers predict the label of a new observation based on proximity to a known observation.\n",
    "\n",
    "More generalized classifiers attempt to find dividing surfaces within the n-dimesional feature space to decide how to label a new observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metrics and scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading on classification metrics:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/model_evaluation.html#confusion-matrix\n",
    "\n",
    "http://scikit-learn.org/stable/modules/model_evaluation.html#classification-report\n",
    "\n",
    "http://scikit-learn.org/stable/modules/model_evaluation.html#precision-recall-and-f-measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics for binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a few different classifiers so we can compare them. The dataset seeks to model whether a bank customer will respond to a marketing campaign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv(\"data/bank-campaign.csv\")\n",
    "X = data.drop(columns=\"target\").values\n",
    "y = data.target.values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DummClassifier will *always* assign the same label to every sample. Most people responded \"no\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "88% seems like a good score, no?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy_majority = DummyClassifier(strategy='most_frequent').fit(X_train, y_train)\n",
    "pred_most_frequent = dummy_majority.predict(X_test)\n",
    "\n",
    "print(\"predicted labels: %s\" % np.unique(pred_most_frequent))\n",
    "print(\"score: %f\" % dummy_majority.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build some more sophisticated models. These have similar scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=2).fit(X_train, y_train)\n",
    "pred_tree = tree.predict(X_test)\n",
    "\n",
    "tree.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(C=0.1, solver='liblinear').fit(X_train, y_train)\n",
    "pred_logreg = logreg.predict(X_test)\n",
    "\n",
    "print(\"logreg score: %f\" % logreg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrices display the total amount of true and false positive and negative classifcations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.plots.plot_binary_confusion_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion = confusion_matrix(y_test, pred_logreg)\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fractions may be more useful to compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_no = y_test[y_test=='no'].shape[0]\n",
    "n_yes = y_test[y_test=='yes'].shape[0]\n",
    "\n",
    "sizes = np.array([n_no, n_yes])\n",
    "\n",
    "confusion / sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did the other models do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Most frequent class:\")\n",
    "print(confusion_matrix(y_test, pred_most_frequent)/sizes)\n",
    "print(\"\\nDecision tree:\")\n",
    "print(confusion_matrix(y_test, pred_tree)/sizes)\n",
    "print(\"\\nLogistic Regression\")\n",
    "print(confusion_matrix(y_test, pred_logreg)/sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relation to accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision, recall and f-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false
   },
   "source": [
    "\\begin{equation}\n",
    "\\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\text{F1} = 2 \\cdot \\frac{\\text{precision} \\cdot \\text{recall}}{\\text{precision} + \\text{recall}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print(\"f1 score most frequent: %.2f\" % f1_score(y_test, pred_most_frequent, pos_label=\"yes\"))\n",
    "print(\"f1 score tree: %.2f\" % f1_score(y_test, pred_tree, pos_label=\"yes\"))\n",
    "print(\"f1 score: %.2f\" % f1_score(y_test, pred_logreg, pos_label=\"yes\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, pred_most_frequent,\n",
    "                            target_names=[\"no\", \"yes\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DeicisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pred_tree,\n",
    "                            target_names=[\"no\", \"yes\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pred_logreg,\n",
    "                            target_names=[\"no\", \"yes\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taking uncertainty into account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default *decision threshold* is the value at which the classifier distinguishes between False and True. This threshold can be modified if you wish to optimize either false positives or false negatives.\n",
    "\n",
    "For SVC models a *decision function* is provided that takes the raw probabilities of each target value and provides a linear function to destinguish between True and False. By default the decision threshold is set to 0. Predicted values greater than 0 are assigned True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.plots.plot_decision_threshold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.mglearn.datasets import make_blobs \n",
    "from sklearn.svm import SVC\n",
    "\n",
    "X, y = make_blobs(n_samples=(400, 50), centers=2, cluster_std=[7.0, 2],        \n",
    "                  random_state=22)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "svc = SVC(gamma=.05).fit(X_train, y_train)                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, svc.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the Classification Report when the threshold is lowered to 0.8. Precision for the False class is now perfect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lower_threshold = svc.decision_function(X_test) > -.8\n",
    "\n",
    "print(classification_report(y_test, y_pred_lower_threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision-Recall curves and ROC curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curves allow us to view tradeoff between 1) precision and recall or 2) true positive rate versus false positive rate (ROC).\n",
    "\n",
    "http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html\n",
    "\n",
    "also want to look at:\n",
    "\n",
    "http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html#example-model-selection-plot-roc-crossval-py\n",
    "\n",
    "Precision-Recall curves are best used when you *imbalanced classes*, like the Cancer problem above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a similar dataset as before, but with more samples to get a smoother curve\n",
    "X, y = make_blobs(n_samples=(4000, 500), centers=2, cluster_std=[7.0, 2], random_state=22)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "svc = SVC(gamma=.05).fit(X_train, y_train)\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(\n",
    "    y_test, svc.decision_function(X_test))\n",
    "# find threshold closest to zero:\n",
    "close_zero = np.argmin(np.abs(thresholds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_recall = pd.DataFrame({\n",
    "    'precision': precision,\n",
    "    'recall': recall,\n",
    "    'threshold': np.append(thresholds, np.nan)\n",
    "})\n",
    "\n",
    "precision_recall_plot = precision_recall.hvplot.line(x='precision', y='recall', hover_cols=['threshold'], width=800,\n",
    "                                                     label='SVC')\n",
    "zero = hv.Points((precision[close_zero], recall[close_zero]), label='Zero').options(marker='o', color='black',\n",
    "                                                                                    size=10, fill_color=None)\n",
    "svc_plot = precision_recall_plot * zero\n",
    "svc_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For RandomForestClassifier probabilities between 0 and 1 are provided. The default threshold is then 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=0, max_features=2)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "precision_rf, recall_rf, thresholds_rf = precision_recall_curve(\n",
    "    y_test, rf.predict_proba(X_test)[:, 1])\n",
    "\n",
    "close_default_rf = np.argmin(np.abs(thresholds_rf - 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_recall_rf = pd.DataFrame({\n",
    "    'precision': precision_rf,\n",
    "    'recall': recall_rf,\n",
    "    'threshold': np.append(thresholds_rf, np.nan)\n",
    "})\n",
    "\n",
    "pr_rf = precision_recall_rf.hvplot.line(x='precision', y='recall', hover_cols=['threshold'], width=800,\n",
    "                                       label='RandomForest')\n",
    "point5_rf = (hv.Points((precision_rf[close_default_rf], recall_rf[close_default_rf]), label='0.5')\n",
    "             .options(marker='o', color='red', size=10, fill_color=None))\n",
    "\n",
    "svc_plot * pr_rf * point5_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"f1_score of random forest: %f\" % f1_score(y_test, rf.predict(X_test)))\n",
    "print(\"f1_score of svc:           %f\" % f1_score(y_test, svc.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "ap_rf = average_precision_score(y_test, rf.predict_proba(X_test)[:, 1])\n",
    "ap_svc = average_precision_score(y_test, svc.decision_function(X_test))\n",
    "\n",
    "print(\"average precision of random forest: %f\" % ap_rf)\n",
    "print(\"average precision of svc:           %f\" % ap_svc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: compare cancer models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the RandomForest `rnd` and LogisticRegression `logreg` cancer models fitted for you, construct confusion matrices with the argument `labels=['malignant','benign']`.\n",
    "\n",
    "Would we me more concerned in improving *preicision* or *recall*? Remember that False means no-cancer. Use `precision_score()` or `recall_score()` with `pos_label='benign'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "_cancer = load_breast_cancer()\n",
    "cancer = pd.DataFrame(data=_cancer.data, columns=_cancer.feature_names)\n",
    "cancer['target'] = _cancer.target\n",
    "cancer['target'] = cancer['target'].replace({0:'malignant', 1:'benign'})\n",
    "cancer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import recall_score, precision_score\n",
    "\n",
    "X = cancer.drop(columns='target').values\n",
    "y = cancer['target'].values\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, random_state=42)\n",
    "\n",
    "logreg = LogisticRegression(solver='liblinear', C=1)\n",
    "logreg.fit(Xtrain, ytrain)\n",
    "\n",
    "bayes = GaussianNB()\n",
    "bayes.fit(Xtrain, ytrain)\n",
    "\n",
    "print(f\"LogisticRegression score: {logreg.score(Xtest,ytest):.4f}\")\n",
    "print(f\"Naive Bayes        score: {bayes.score(Xtest,ytest):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR SOLUTION HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<button data-toggle=\"collapse\" data-target=\"#cancer\" class='btn btn-primary'>Show solution</button>\n",
    "\n",
    "<div id=\"cancer\" class=\"collapse\">\n",
    "\n",
    "High *false negatives* can put people at risk! False positives can be ignored (somewhat). You want a model that **maximizes recall** near 1.0.\n",
    "\n",
    "Logistic Regression has a *slightly* higher precision score.\n",
    "\n",
    "\n",
    " ```python\n",
    "print(\"Logistic Regression\")\n",
    "print(confusion_matrix(ytest, logreg.predict(Xtest), labels=['malignant', 'benign']))\n",
    "print(recall_score(ytest, logreg.predict(Xtest), pos_label='benign'))\n",
    "\n",
    "print(\"Naive Bayes\")\n",
    "print(confusion_matrix(ytest, bayes.predict(Xtest), labels=['malignant', 'benign']))\n",
    "print(recall_score(ytest, bayes.predict(Xtest), pos_label='benign'))\n",
    " ```\n",
    " \n",
    "Extra: The plots look weird. Why is that?\n",
    "\n",
    " ```python\n",
    "plots = []\n",
    "\n",
    "for model in logreg, bayes:\n",
    "    p, r, t = precision_recall_curve(ytest, model.predict_proba(Xtest)[:, 1], pos_label='benign')\n",
    "    close_default = np.argmin(np.abs(t - 0.5))\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'precision': p,\n",
    "        'recall': r,\n",
    "        'threshold': np.append(t, np.nan)\n",
    "    })\n",
    "    \n",
    "    curve = df.hvplot.line(x='precision', y='recall', hover_cols=['threshold'], width=800,\n",
    "                                       label=model.__class__.__name__, padding=0.02)\n",
    "    point = (hv.Points((p[close_default], r[close_default]), label=f'default {model.__class__.__name__}')\n",
    "             .options(marker='o', size=10, fill_color=None))\n",
    "    \n",
    "    plots.append(curve * point)\n",
    "\n",
    "plots[0] * plots[1]\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC and AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at Receiver Operating Characteristic (ROC) plots, it is also helpful to calculate the Area Under the Curve (AUC). A perfect classifier has an AUC of 1.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to precision-recall curves, ROC uses *false positive rate (FPR)* and *true positive rate (TPR)*. ROC is best used for problems where the classes are well balanced.\n",
    "\n",
    "The false positive rate is defined as\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{FPR} = \\frac{\\text{FP}}{\\text{FP} + \\text{TN}}\n",
    "\\end{equation}\n",
    "\n",
    "The true positive rate is defined as\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{TPR} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a similar dataset as before, but with balanced classes\n",
    "X, y = make_blobs(n_samples=4500, centers=2, cluster_std=[7.0, 2], random_state=22)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "svc = SVC(gamma=.05).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, svc.decision_function(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">the top left corner of the plot is the “ideal” point - a false positive rate of zero, and a true positive rate of one. This is not very realistic, but it does mean that a larger area under the curve (AUC) is usually better.\n",
    "\n",
    ">The “steepness” of ROC curves is also important, since it is ideal to maximize the true positive rate while minimizing the false positive rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_svc = pd.DataFrame({\n",
    "    'FPR (precision)': fpr,\n",
    "    'TPR (recall)': tpr,\n",
    "    'threshold': thresholds\n",
    "})\n",
    "\n",
    "roc_svc_plot = roc_svc.hvplot.line(x='FPR (precision)', y='TPR (recall)', hover_cols=['threshold'], width=800,\n",
    "                            label='SVC')\n",
    "\n",
    "close_zero = np.argmin(np.abs(thresholds))\n",
    "zero_svc = (hv.Points((fpr[close_zero], tpr[close_zero]), label='Zero (svc)')\n",
    "             .options(marker='o', size=10, fill_color=None))\n",
    "\n",
    "roc_svc_plot * zero_svc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare SVC to the RandomForest model\n",
    "\n",
    "**Note**: the diagonal line represents a *random guess* classifier. Good classifiers have ROC curves significantly above the diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, rf.predict_proba(X_test)[:, 1])\n",
    "\n",
    "\n",
    "roc_rf = pd.DataFrame({\n",
    "    'FPR (precision)': fpr_rf,\n",
    "    'TPR (recall)': tpr_rf,\n",
    "    'threshold': thresholds_rf\n",
    "})\n",
    "\n",
    "roc_rf_plot = roc_rf.hvplot.line(x='FPR (precision)', y='TPR (recall)', hover_cols=['threshold'], width=800,\n",
    "                            label='RandomForest', title='ROC')\n",
    "\n",
    "close_default_rf = np.argmin(np.abs(thresholds_rf - 0.5))\n",
    "point5_rf = (hv.Points((fpr_rf[close_default_rf], tpr_rf[close_default_rf]), label='0.5 (rf)')\n",
    "             .options(marker='o', size=10, fill_color=None))\n",
    "\n",
    "diag = hv.Curve([(0,0),(1,1)]).options(color='black', alpha=0.2)\n",
    "\n",
    "roc_svc_plot * zero_svc * roc_rf_plot * point5_rf * diag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the area (AUC) provides a single metric to evaluate different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "rf_auc = roc_auc_score(y_test, rf.predict_proba(X_test)[:, 1])\n",
    "svc_auc = roc_auc_score(y_test, svc.decision_function(X_test))\n",
    "\n",
    "print(\"AUC for Random Forest: %f\" % rf_auc)\n",
    "print(\"AUC for SVC:           %f\" % svc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC is also sensitive to changes in hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/bank-campaign.csv\")\n",
    "X = data.drop(columns=\"target\").values\n",
    "y = data.target.values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=0, train_size=.1, test_size=.1)\n",
    "\n",
    "plots = []\n",
    "for gamma in [1, 0.01, 0.001]:\n",
    "    svc = SVC(gamma=gamma).fit(X_train, y_train)\n",
    "    \n",
    "    accuracy = svc.score(X_test, y_test)\n",
    "    auc = roc_auc_score(y_test == \"yes\", svc.decision_function(X_test))\n",
    "    print(\"gamma = %.03f  accuracy = %.02f  AUC = %.02f\" % (gamma, accuracy, auc))\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_test , svc.decision_function(X_test), pos_label=\"yes\")\n",
    "    plots.append(hv.Curve((fpr,tpr), 'FPR', 'TPR', label=f'gamma={gamma:.03f}'))\n",
    "\n",
    "(plots[0] * plots[1] * plots[2]).options(width=500, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a binary classification problem, the confusion matrix has a shape of `(2, 2)`.  In a multi-class problem, the confusion matrix has a shape `(n_classes, n_classes)` to show the correctly vs incorrectly classified counts for each class versus every other class.\n",
    "\n",
    "A perfect confusion matrix has positive main diagonal and zeros elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    digits.data, digits.target, random_state=0)\n",
    "\n",
    "lr = LogisticRegression(solver='liblinear', multi_class='auto').fit(X_train, y_train)\n",
    "pred = lr.predict(X_test)\n",
    "\n",
    "mat = confusion_matrix(y_test, pred)\n",
    "\n",
    "print(\"accuracy: %0.3f\" % accuracy_score(y_test, pred))\n",
    "print(\"confusion matrix:\")\n",
    "print(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"micro average f1 score: %f\" % f1_score(y_test, pred, average=\"micro\"))\n",
    "print(\"macro average f1 score: %f\" % f1_score(y_test, pred, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we reviewed the following topics in preparation for more advanced topics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * [Evaluation metrics and scoring](#Evaluation-Metrics-and-scoring)\n",
    " * [Metrics for binary classification](#Metrics-for-binary-classification)\n",
    " * [Confusion matrices](#Confusion-matrices)\n",
    " * [Precision, recall and f-score](#Precision,-recall-and-f-score)\n",
    " * [Precision-Recall curves and ROC curves](#Precision-Recall-curves-and-ROC-curves)\n",
    " * [ROC and AUC](#Receiver-Operating-Characteristics-%28ROC%29-and-AUC)\n",
    " * [Multi-class classification](#Multi-class-classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='Evaluation_Metrics_Exercises.ipynb' class='btn btn-primary btn-lg'>Exercises</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/copyright.png'>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 [module-machine-learning]",
   "language": "python",
   "name": "anaconda-project-module-machine-learning-python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
